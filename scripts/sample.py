import argparse
import os
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from pathlib import Path
import datetime
from tqdm import tqdm

from Unet import ControlPointUNet
from diffusion import CreepDiffusionSampler, create_sampler
from forward import CreepDeformationEngine


class BambooRestorer:
    """ç«¹ç®€æ¢å¤å™¨ - ä¸»è¦çš„æ¨ç†æ¥å£"""
    
    def __init__(self, model_path, device='cuda', T=100):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.T = T
        self.model = self._load_model(model_path)
        self.sampler = create_sampler(self.model, T=T)
        self.sampler.to(self.device)
  
    
    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # ä»æ£€æŸ¥ç‚¹ä¸­è·å–æ¨¡å‹å‚æ•°
        if 'args' in checkpoint:
            args = checkpoint['args']
            model = ControlPointUNet(
                img_channels=args.img_channels,
                base_channels=args.base_channels,
                control_grid_size=(args.control_nx, args.control_ny),
                channel_mults=args.channel_mults,
                num_res_blocks=args.num_res_blocks,
                time_emb_dim=args.time_emb_dim,
                time_emb_scale=args.time_emb_scale,
                num_classes=args.num_classes if args.num_classes > 0 else None,
                activation=torch.nn.functional.relu,
                dropout=args.dropout,
                attention_resolutions=args.attention_resolutions,
                norm=args.norm,
                num_groups=args.num_groups,
                max_displacement=args.max_displacement,
                image_size=args.image_size,
            )
        else:
            # ä½¿ç”¨é»˜è®¤å‚æ•°åˆ›å»ºæ¨¡å‹
            print("âš ï¸ æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰æ‰¾åˆ°argsï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            model = ControlPointUNet(
                img_channels=3,
                base_channels=64,
                control_grid_size=(4, 2),
                channel_mults=[1, 2, 4, 8],
                num_res_blocks=2,
                time_emb_dim=256,
                time_emb_scale=1.0,
                num_classes=None,
                activation=torch.nn.functional.relu,
                dropout=0.1,
                attention_resolutions=[1, 2],
                norm="gn",
                num_groups=32,
                max_displacement=50.0,
                image_size=(640, 64),
            )
        
        # åŠ è½½æƒé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        return model
    
    def load_image(self, image_path, target_size=(640, 64)):
        """åŠ è½½å¹¶é¢„å¤„ç†ç«¹ç®€å›¾åƒ"""
        try:
            # åŠ è½½å›¾åƒ
            pil_image = Image.open(image_path).convert('RGB')
            
            # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
            pil_image = pil_image.resize((target_size[1], target_size[0]), Image.LANCZOS)
            
            # è½¬æ¢ä¸ºtensor
            img_array = np.array(pil_image, dtype=np.float32) / 255.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
            
            return img_tensor.to(self.device), pil_image
            
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½å›¾åƒ {image_path}: {e}")
            return None, None
    
    def restore_single_image(self, deformed_image, show_progress=True):
        """æ¢å¤å•å¼ ç«¹ç®€å›¾åƒ"""
        with torch.no_grad():
            if show_progress:
                print(f"ğŸ”„ å¼€å§‹æ¢å¤ç«¹ç®€...")
            
            # ä½¿ç”¨é‡‡æ ·å™¨è¿›è¡Œæ¢å¤
            restored_image, history = self.sampler(deformed_image, show_progress=show_progress)
            
            if show_progress:
                print(f"âœ… æ¢å¤å®Œæˆ!")
            
            return restored_image, history
    
    def restore_images_batch(self, image_paths, output_dir, batch_size=1):
        """æ‰¹é‡æ¢å¤ç«¹ç®€å›¾åƒ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        restored_dir = output_dir / "restored"
        comparisons_dir = output_dir / "comparisons"
        history_dir = output_dir / "history"
        
        for dir_path in [restored_dir, comparisons_dir, history_dir]:
            dir_path.mkdir(exist_ok=True)
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡æ¢å¤ {len(image_paths)} å¼ ç«¹ç®€å›¾åƒ")
        
        for i, image_path in enumerate(tqdm(image_paths, desc="æ¢å¤è¿›åº¦")):
            image_name = Path(image_path).stem
            
            # åŠ è½½å›¾åƒ
            deformed_tensor, original_pil = self.load_image(image_path)
            if deformed_tensor is None:
                continue
            
            # æ¢å¤å›¾åƒ
            restored_tensor, history = self.restore_single_image(deformed_tensor, show_progress=False)
            
            # ä¿å­˜æ¢å¤ç»“æœ
            self._save_restoration_results(
                deformed_tensor[0], restored_tensor[0], history,
                image_name, restored_dir, comparisons_dir, history_dir
            )
            
            print(f"âœ… å®Œæˆ {i+1}/{len(image_paths)}: {image_name}")
        
        print(f"ğŸ‰ æ‰¹é‡æ¢å¤å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
        return output_dir
    
    def _save_restoration_results(self, deformed, restored, history, 
                                image_name, restored_dir, comparisons_dir, history_dir):
        """ä¿å­˜æ¢å¤ç»“æœ"""
        
        # 1. ä¿å­˜æ¢å¤åçš„å›¾åƒ
        restored_img = torch.clamp(restored, 0, 1)
        torchvision.utils.save_image(
            restored_img, 
            restored_dir / f"{image_name}_restored.png"
        )
        
        # 2. ä¿å­˜å¯¹æ¯”å›¾
        comparison = torch.cat([deformed, restored_img], dim=2)  # æ°´å¹³æ‹¼æ¥
        torchvision.utils.save_image(
            comparison, 
            comparisons_dir / f"{image_name}_comparison.png"
        )
        
        # 3. ä¿å­˜æ¢å¤å†å²ï¼ˆå¯é€‰ï¼Œå±•ç¤ºæ¢å¤è¿‡ç¨‹ï¼‰
        if len(history) > 1:
            # é€‰æ‹©å‡ ä¸ªå…³é”®å¸§
            key_frames = [0, len(history)//4, len(history)//2, 3*len(history)//4, -1]
            selected_frames = [history[i] for i in key_frames if i < len(history)]
            
            if selected_frames:
                # æ°´å¹³æ‹¼æ¥å…³é”®å¸§
                frames_tensor = torch.cat([torch.clamp(frame[0], 0, 1) for frame in selected_frames], dim=2)
                torchvision.utils.save_image(
                    frames_tensor, 
                    history_dir / f"{image_name}_process.png"
                )


def create_detailed_visualization(deformed, restored, history, save_path):
    """åˆ›å»ºè¯¦ç»†çš„å¯è§†åŒ–ç»“æœ"""
    fig, axes = plt.subplots(3, min(len(history), 6), figsize=(18, 12))
    if len(history) < 6:
        # å¦‚æœå†å²å¸§æ•°å°‘äº6ï¼Œè°ƒæ•´å­å›¾
        fig, axes = plt.subplots(3, len(history), figsize=(3*len(history), 12))
    
    # ç¡®ä¿axesæ˜¯2Dæ•°ç»„
    if axes.ndim == 1:
        axes = axes.reshape(3, -1)
    
    # é€‰æ‹©è¦å±•ç¤ºçš„å¸§
    num_display = min(axes.shape[1], len(history))
    selected_indices = np.linspace(0, len(history)-1, num_display, dtype=int)
    
    for i, idx in enumerate(selected_indices):
        if i >= axes.shape[1]:
            break
            
        frame = history[idx][0]  # å–ç¬¬ä¸€ä¸ªbatch
        
        # ç¬¬ä¸€è¡Œï¼šåŸå›¾å’Œå½“å‰æ¢å¤çŠ¶æ€
        if i == 0:
            img_display = deformed.permute(1, 2, 0).cpu().numpy()
            axes[0, i].imshow(np.clip(img_display, 0, 1))
            axes[0, i].set_title(f'å˜å½¢ç«¹ç®€\n(è¾“å…¥)')
        else:
            img_display = torch.clamp(frame, 0, 1).permute(1, 2, 0).cpu().numpy()
            axes[0, i].imshow(img_display)
            progress = idx / (len(history) - 1) * 100
            axes[0, i].set_title(f'æ¢å¤è¿›åº¦\n{progress:.1f}%')
        axes[0, i].axis('off')
        
        # ç¬¬äºŒè¡Œï¼šå·®å¼‚å›¾
        if i > 0:
            diff = torch.abs(frame - deformed).mean(dim=0).cpu().numpy()
            im = axes[1, i].imshow(diff, cmap='hot', aspect='auto')
            axes[1, i].set_title(f'å˜åŒ–å¼ºåº¦')
            plt.colorbar(im, ax=axes[1, i], fraction=0.046)
        else:
            axes[1, i].text(0.5, 0.5, 'åŸå§‹çŠ¶æ€', ha='center', va='center', 
                          transform=axes[1, i].transAxes)
        axes[1, i].axis('off')
        
        # ç¬¬ä¸‰è¡Œï¼šæ¢å¤è´¨é‡æŒ‡æ ‡
        if i > 0:
            # è®¡ç®—PSNR
            mse = torch.mean((frame - deformed) ** 2).item()
            psnr = -10 * np.log10(mse + 1e-8) if mse > 0 else float('inf')
            
            axes[2, i].bar(['PSNR'], [psnr], color='skyblue')
            axes[2, i].set_title(f'PSNR: {psnr:.1f}dB')
            axes[2, i].set_ylim(0, max(30, psnr * 1.1))
        else:
            axes[2, i].text(0.5, 0.5, 'è´¨é‡æŒ‡æ ‡', ha='center', va='center', 
                          transform=axes[2, i].transAxes)
        axes[2, i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def main():
    parser = argparse.ArgumentParser(description='ç«¹ç®€æ¢å¤é‡‡æ ·å™¨')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--model_path', type=str, required=True,
                      help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (.ptæ–‡ä»¶)')
    parser.add_argument('--input_path', type=str, required=True,
                      help='è¾“å…¥å›¾åƒè·¯å¾„æˆ–ç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True,
                      help='è¾“å‡ºç›®å½•')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--device', type=str, default='cuda',
                      choices=['cuda', 'cpu'], help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--T', type=int, default=100,
                      help='æ‰©æ•£æ­¥æ•°')
    parser.add_argument('--batch_size', type=int, default=1,
                      help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--image_size', type=int, nargs=2, default=[640, 64],
                      help='å›¾åƒå°ºå¯¸ [H, W]')
    parser.add_argument('--create_visualization', action='store_true',
                      help='åˆ›å»ºè¯¦ç»†çš„å¯è§†åŒ–ç»“æœ')
    parser.add_argument('--save_history', action='store_true',
                      help='ä¿å­˜æ¢å¤è¿‡ç¨‹å†å²')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    input_path = Path(args.input_path)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return
    
    # åˆ›å»ºæ¢å¤å™¨
    restorer = BambooRestorer(
        model_path=args.model_path,
        device=args.device,
        T=args.T
    )
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(args.output_dir) / f"restoration_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†å›¾åƒè·¯å¾„
    if input_path.is_file():
        # å•ä¸ªæ–‡ä»¶
        image_paths = [str(input_path)]
        print(f"ğŸ“‚ å¤„ç†å•ä¸ªæ–‡ä»¶: {input_path}")
    else:
        # ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒ
        valid_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']
        image_paths = []
        for ext in valid_extensions:
            image_paths.extend(input_path.glob(f"*{ext}"))
            image_paths.extend(input_path.glob(f"*{ext.upper()}"))
        
        image_paths = [str(p) for p in image_paths]
        print(f"ğŸ“‚ æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒæ–‡ä»¶")
    
    if not image_paths:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶")
        return
    
    # å¼€å§‹æ¢å¤
    try:
        if len(image_paths) == 1:
            # å•å›¾åƒå¤„ç†
            image_path = image_paths[0]
            image_name = Path(image_path).stem
            
            print(f"ğŸ”„ å¤„ç†å•å¼ å›¾åƒ: {image_name}")
            
            # åŠ è½½å¹¶æ¢å¤
            deformed_tensor, _ = restorer.load_image(image_path, tuple(args.image_size))
            if deformed_tensor is not None:
                restored_tensor, history = restorer.restore_single_image(deformed_tensor)
                
                # ä¿å­˜ç»“æœ
                restorer._save_restoration_results(
                    deformed_tensor[0], restored_tensor[0], history,
                    image_name, output_dir, output_dir, output_dir
                )
                
                # åˆ›å»ºè¯¦ç»†å¯è§†åŒ–
                if args.create_visualization:
                    print("ğŸ¨ åˆ›å»ºè¯¦ç»†å¯è§†åŒ–...")
                    create_detailed_visualization(
                        deformed_tensor[0], restored_tensor[0], history,
                        output_dir / f"{image_name}_detailed.png"
                    )
                
                print(f"âœ… å•å›¾åƒæ¢å¤å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {output_dir}")
            
        else:
            # æ‰¹é‡å¤„ç†
            restorer.restore_images_batch(
                image_paths=image_paths,
                output_dir=output_dir,
                batch_size=args.batch_size
            )
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ¢å¤è¿‡ç¨‹æå‰ç»“æŸ")
    except Exception as e:
        print(f"âŒ æ¢å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"ğŸ‰ ç¨‹åºå®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    main()
