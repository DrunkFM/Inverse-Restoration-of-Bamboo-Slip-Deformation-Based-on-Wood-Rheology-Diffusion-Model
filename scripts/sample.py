import argparse
import os
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from pathlib import Path
import datetime
from tqdm import tqdm

# ç¡®ä¿ä»æ‚¨çš„é¡¹ç›®ä¸­æ­£ç¡®å¯¼å…¥Unet
from Unet import ControlPointUNet


class BambooRestorer:
    """
    ç«¹ç®€æ¢å¤å™¨ - å¤šæ­¥è¿­ä»£ä¿®å¤ç‰ˆæœ¬
    """

    def __init__(self, model_path, device='cuda', T=30):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.T = T  # æ‰©æ•£/æ¢å¤çš„æ€»æ­¥æ•°
        self.model = self._load_model(model_path)
        print(f"ğŸ”¬ å¤šæ­¥æ¢å¤å™¨åˆå§‹åŒ–å®Œæˆï¼Œå°†åœ¨è®¾å¤‡ {self.device} ä¸Šè¿è¡Œ {self.T} æ­¥")

    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            raise

        # ä»æ£€æŸ¥ç‚¹ä¸­è·å–æ¨¡å‹å‚æ•°æ¥æ„å»ºæ¨¡å‹
        if 'args' in checkpoint:
            args = checkpoint['args']
            print("âœ… ä»æ£€æŸ¥ç‚¹ä¸­æ‰¾åˆ°'args'ï¼Œä½¿ç”¨ä¿å­˜çš„å‚æ•°æ„å»ºæ¨¡å‹ã€‚")
            model = ControlPointUNet(
                img_channels=args.img_channels,
                base_channels=args.base_channels,
                control_grid_size=(args.control_nx, args.control_ny),
                channel_mults=args.channel_mults,
                num_res_blocks=args.num_res_blocks,
                time_emb_dim=args.time_emb_dim,
                time_emb_scale=args.time_emb_scale,
                num_classes=args.num_classes if args.num_classes > 0 else None,
                activation=torch.nn.functional.relu,
                dropout=args.dropout,
                attention_resolutions=args.attention_resolutions,
                norm=args.norm,
                num_groups=args.num_groups,
                max_displacement=args.max_displacement,
                image_size=args.image_size,
            )
        else:
            # å¦‚æœæ£€æŸ¥ç‚¹æ²¡æœ‰ä¿å­˜å‚æ•°ï¼Œåˆ™ä½¿ç”¨ä¸train.pyåŒ¹é…çš„é»˜è®¤å€¼
            print("âš ï¸ æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰æ‰¾åˆ°'args'ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°æ„å»ºæ¨¡å‹ã€‚")
            print("   >>> ç¡®ä¿è¿™äº›é»˜è®¤å‚æ•°ä¸æ‚¨çš„è®­ç»ƒè®¾ç½®å®Œå…¨ä¸€è‡´! <<<")
            model = ControlPointUNet(
                img_channels=3,
                base_channels=64,
                control_grid_size=(32, 4),
                channel_mults=[1, 2, 4, 8],
                num_res_blocks=2,
                time_emb_dim=256,
                time_emb_scale=1.0,
                num_classes=None,
                activation=torch.nn.functional.relu,
                dropout=0.1,
                attention_resolutions=[1, 2],
                norm="gn",
                num_groups=32,
                max_displacement=50.0,
                image_size=(640, 24),
            )

        # åŠ è½½æ¨¡å‹æƒé‡
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            # å…¼å®¹åªä¿å­˜äº†æ¨¡å‹æƒé‡çš„æ—§æ ¼å¼
            model.load_state_dict(checkpoint)

        model.to(self.device)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸå¹¶ç§»è‡³ {self.device}")

        return model

    def load_image(self, image_path, target_size=(640, 64)):
        """åŠ è½½å¹¶é¢„å¤„ç†å•å¼ ç«¹ç®€å›¾åƒ"""
        try:
            pil_image = Image.open(image_path).convert('RGB')
            # PILçš„resizeéœ€è¦(å®½åº¦, é«˜åº¦)
            pil_image = pil_image.resize((target_size[1], target_size[0]), Image.LANCZOS)

            img_array = np.array(pil_image, dtype=np.float32) / 255.0
            # è½¬æ¢ä¸º (C, H, W) æ ¼å¼å¹¶å¢åŠ batchç»´åº¦ (1, C, H, W)
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)

            return img_tensor.to(self.device)

        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½å›¾åƒ {image_path}: {e}")
            return None

    def restore_single_image(self, deformed_image_tensor):
        """
        æ¢å¤å•å¼ ç«¹ç®€å›¾åƒ - é‡‡ç”¨å¤šæ­¥è¿­ä»£ä¿®å¤
        """
        with torch.no_grad():
            x_t = deformed_image_tensor.clone()
            history = [x_t.clone()]  # ä¿å­˜å†å²è®°å½•ç”¨äºå¯è§†åŒ–

            # ä»æ—¶é—´æ­¥ T è¿­ä»£åˆ° 1
            progress_bar = tqdm(reversed(range(1, self.T + 1)), total=self.T, desc="å¤šæ­¥æ¢å¤ä¸­")
            for time_step in progress_bar:
                # åˆ›å»ºå½“å‰æ—¶é—´æ­¥çš„å¼ é‡
                t = torch.full((deformed_image_tensor.shape[0],), time_step, dtype=torch.long, device=self.device)

                # è°ƒç”¨æ¨¡å‹è¿›è¡Œä¸€æ­¥é¢„æµ‹å’Œåº”ç”¨é€†å˜å½¢
                # è¿™ä¸ªæ–¹æ³•å·²ç»å°è£…åœ¨Unet.pyä¸­
                x_t, _ = self.model.predict_and_apply_deformation(x_t, t)

                # ä¸ºäº†å¯è§†åŒ–ï¼Œä¸éœ€è¦æ¯ä¸€æ­¥éƒ½ä¿å­˜ï¼Œåªåœ¨ç‰¹å®šæ­¥éª¤ä¿å­˜
                if time_step % (self.T // 10) == 0 or time_step == 1:
                    history.append(x_t.clone())

                progress_bar.set_postfix({"æ­¥éª¤": f"{time_step}/{self.T}"})

            # æœ€ç»ˆç»“æœè¿›è¡Œclampç¡®ä¿å€¼åœ¨[0, 1]èŒƒå›´å†…
            restored_image = torch.clamp(x_t, 0, 1)

            print("âœ… å¤šæ­¥æ¢å¤å®Œæˆ!")
            return restored_image, history

    @staticmethod
    def _save_restoration_results(deformed_tensor, restored_tensor, history,
                                  image_name, output_dir):
        """
        ä¿å­˜æ¢å¤ç»“æœï¼ŒåŒ…æ‹¬å•å¼ ç»“æœã€å¯¹æ¯”å›¾å’Œè¿‡ç¨‹å›¾
        """
        # å®šä¹‰è¾“å‡ºå­ç›®å½•
        restored_dir = output_dir / "restored"
        comparisons_dir = output_dir / "comparisons"
        history_dir = output_dir / "history"
        for dir_path in [restored_dir, comparisons_dir, history_dir]:
            dir_path.mkdir(exist_ok=True, parents=True)

        # 1. ä¿å­˜æ¢å¤åçš„å›¾åƒ
        torchvision.utils.save_image(
            restored_tensor,
            restored_dir / f"{image_name}_restored.png"
        )

        # 2. ä¿å­˜å¯¹æ¯”å›¾ (å˜å½¢å‰ vs æ¢å¤å)
        comparison_image = torch.cat([deformed_tensor, restored_tensor], dim=2)  # æ°´å¹³æ‹¼æ¥
        torchvision.utils.save_image(
            comparison_image,
            comparisons_dir / f"{image_name}_comparison.png"
        )

        # 3. ä¿å­˜æ¢å¤å†å²è¿‡ç¨‹å›¾
        if history:
            # ä»å†å²è®°å½•ä¸­å‡åŒ€é‡‡æ ·æœ€å¤š10å¼ å›¾åƒè¿›è¡Œå¯è§†åŒ–
            num_frames = len(history)
            if num_frames > 10:
                indices = np.linspace(0, num_frames - 1, 10, dtype=int)
                sampled_history = [history[i] for i in indices]
            else:
                sampled_history = history

            # å°†é‡‡æ ·çš„å†å²å¸§æ°´å¹³æ‹¼æ¥
            history_image = torch.cat([torch.clamp(frame.squeeze(0), 0, 1) for frame in sampled_history], dim=2)
            torchvision.utils.save_image(
                history_image,
                history_dir / f"{image_name}_process.png"
            )


def main():
    parser = argparse.ArgumentParser(description='ç«¹ç®€æ¢å¤é‡‡æ ·å™¨ - å¤šæ­¥æ¢å¤ç‰ˆ')

    parser.add_argument('--model_path', type=str,default=r'D:\python_app\DDPM\best_model\best_model.pt',
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (.ptæ–‡ä»¶)')
    parser.add_argument('--input_path', type=str,default=r'D:\python_app\DDPM\æ‰­æ›²å›¾ç‰‡è½¬åŒ–é›†\1èƒŒï¼ˆ277-02ï¼‰â– åå¹´è³ªæ—¥.png',
                        help='è¾“å…¥å›¾åƒè·¯å¾„æˆ–åŒ…å«å›¾åƒçš„ç›®å½•')
    parser.add_argument('--output_dir', type=str, default=r'D:\python_app\DDPM\ddpm_new\outputs',
                        help='è¾“å‡ºç»“æœçš„æ ¹ç›®å½•')

    # å¯é€‰å‚æ•°
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'],
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--T', type=int, default=40,
                        help='æ¢å¤çš„æ€»æ­¥æ•°')
    parser.add_argument('--image_size', type=int, nargs=2, default=[640, 24],
                        help='å›¾åƒå°ºå¯¸ [é«˜åº¦, å®½åº¦]')

    args = parser.parse_args()

    # --- è·¯å¾„æ£€æŸ¥ ---
    input_path = Path(args.input_path)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return

    model_path = Path(args.model_path)
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return

    # --- åˆå§‹åŒ–æ¢å¤å™¨ ---
    restorer = BambooRestorer(
        model_path=args.model_path,
        device=args.device,
        T=args.T
    )

    # --- å‡†å¤‡è¾“å‡ºç›®å½• ---
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(args.output_dir) / f"restoration_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜åœ¨: {output_dir}")

    # --- æ”¶é›†å›¾åƒè·¯å¾„ ---
    if input_path.is_file():
        image_paths = [input_path]
    else:
        valid_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']
        image_paths = []
        for ext in valid_extensions:
            image_paths.extend(list(input_path.glob(f"**/*{ext}")))

    if not image_paths:
        print("âŒ åœ¨æŒ‡å®šè·¯å¾„ä¸‹æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒæ–‡ä»¶ã€‚")
        return

    print(f"æ‰¾åˆ°äº† {len(image_paths)} å¼ å¾…å¤„ç†çš„å›¾åƒã€‚")

    # --- å¼€å§‹æ‰¹é‡æ¢å¤ ---
    for image_path in image_paths:
        image_name = Path(image_path).stem
        print(f"\n--- æ­£åœ¨å¤„ç†: {image_name} ---")

        # åŠ è½½å›¾åƒ
        deformed_tensor = restorer.load_image(image_path, tuple(args.image_size))
        if deformed_tensor is None:
            continue

        try:
            # æ¢å¤å›¾åƒ
            restored_tensor, history = restorer.restore_single_image(deformed_tensor)

            # ä¿å­˜æ‰€æœ‰ç»“æœ
            BambooRestorer._save_restoration_results(
                deformed_tensor.squeeze(0),
                restored_tensor.squeeze(0),
                history,
                image_name,
                output_dir
            )
            print(f"âœ… å¤„ç†å®Œæˆ: {image_name}")

        except Exception as e:
            import traceback
            print(f"âŒ å¤„ç†å¤±è´¥ {image_name}: {e}")
            traceback.print_exc()
            continue

    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ! ç»“æœå·²ä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    main()
